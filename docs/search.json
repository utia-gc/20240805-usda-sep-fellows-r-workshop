[
  {
    "objectID": "basic.html",
    "href": "basic.html",
    "title": "Basic Analysis",
    "section": "",
    "text": "This page introduces a brief, basic analysis of grass weights using R.\nThe workflow for a basic analysis consists of two main steps:\n\nPreparing the data — Raw data is read into R and processed into data that is in a format that we can analyze.\nAnalyzing the data — Data is visualized to create plots and tables, and a formal statistical analysis is performed.\n\nWe can visualize the steps of this workflow with a graph:\n\n\n\n\n\nflowchart TB\n    data_file[(\"Data file\")]\n    read_data[Read data\\ninto R]\n    wrangle_data[\"Clean/format\\ndata\"]\n    cleaned_data&gt;\"Analysis-ready\\ndata\"]\n\n    subgraph \"Prepare data\"\n        data_file --- read_data --- wrangle_data --&gt; cleaned_data\n    end\n  \n    viz_data[\"Visualize data\"]\n    visualizations&gt;\"Plots &\\ntables\"]\n    stat_ansys[\"Statistical analysis\"]\n    ansys_res&gt;\"Analysis results\"]\n    \n    subgraph \"Analyze data\"\n        cleaned_data --- viz_data --&gt; visualizations\n        cleaned_data --- stat_ansys --&gt; ansys_res\n    end\n\n\n\n\n\n\n\n\n\nThis section contains the first of the main steps laid out above: preparing the data for analysis.\nThe goal of this section is to take a raw data file and process it in R so that we have data available in our R session that we can directly use as input for our exploratory and formal data analysis.\n\n\n\nRead data\n\nThe process of loading data from a file, database, or other location meant for long term storage into a short-lived form that we work with in a program.\n\n\nI have made the raw data file in available online on a code and data hosting platform called GitHub at the following URL: https://github.com/utia-gc/20240805-usda-sep-fellows-r-workshop/raw/main/data/b_only.csv. By looking at the file name (grasses.csv) and checking the data ourselves, we can see that this file is in comma separated value (CSV) format.\n\n\n\n\n\n\nWarning\n\n\n\nUsing a file available on the internet is convenient for a task like this workshop. R can directly read certain types of files from the internet, so we don’t have to worry about downloading files, finding where they downloaded on everyone’s computer, and reading them in from the correct location.\nHowever, managing downloads and finding data on your computer is an important part of performing analysis on your own, and it can be tricky for beginners. I recommend that you develop a system for keeping track of all of your data files associated with each project. Personally, I prefer having different folders on my computer for each project, and inside of each project folder I have a data/ folder where I keep all of my data files. But that is just my preference, and you may prefer a different setup. What’s important is finding a way that works for you!\n\n\n\n# read b_only data as csv from URL\ncombined &lt;- read.csv(\"https://github.com/utia-gc/20240805-usda-sep-fellows-r-workshop/raw/main/data/grasses.csv\")\n\nb_only &lt;- combined[combined$PERSON == \"Boussad\", ]\n\n\n\n\n\nClean data\n\nThe act of transforming data from its current format into the structure needed for the intended analysis. This can also include data processing steps such as dropping or filling in missing values, computing new values from the data, etc.\n\n\nOur data is actually already in the format needed for our downstream analyses, so our cleaning steps here are rather light. Mainly, they just help us get categorical variables set up in the order we want for plotting and statistical testing.\nR encodes categorical variables in what it calls factors. These factors are inherently ordered – oftentimes this order doesn’t matter, but it controls what our “reference level” is in some statistical tests. This won’t change the results themselves, but we have to be careful about interpreting the results correctly in terms of the reference level.\n\n\n\n\n\n\nEncoding factors\n\n\n\nWhen we don’t care about the reference level of a factor, we can make the data a factor with the as.factor() function and let R take care of setting the levels for us.\nHowever, when we do care about the reference level, we should set the levels ourselves with the factor() function and make sure to use the levels argument.\n\n\n\n# recode categorical variables as factors one column at a time\nb_only$PERSON &lt;- as.factor(b_only$PERSON)\nb_only$GRASS &lt;- as.factor(b_only$GRASS)\nb_only$BLOCK &lt;- as.factor(b_only$BLOCK)\n\nb_only$TREATMENT &lt;- factor(b_only$TREATMENT, levels = c(\"Control\", \"Dry\", \"Liq\", \"Nod\"))\n\nThis is a good place to talk generally about data cleaning and how to check the data to make sure it’s in the structure that we need.\nTo start, we have to know what format we have, and what format we need. There is no “one size fits all” data structure that we need; different analyses require input data to be structured in different ways. A key part of working with data in R is knowing how to process data from the structure that you have into the structure that you need. This means you must work through tutorials and read the documentation for the analysis tools you work with.\nHowever, one general structure that is very common for analyzing data in R is the data.frame. This is the format of the data that we have read in, and it’s what many functions take as input data.\n\n\n\n\n\n\nVisualizing data in a data.frame\n\n\n\nThe most direct way to figure out how your data is structured is to simply look at it. When you’re interactively performing an analysis in RStudio, I recommend two simple ways of looking at your data by running the following in the Console pane:\n# show the contents of the data.frame directly in the Console\nb_only\n\n# open an interactive view of the data.frame as a table\nView(b_only)\n\n\n\n\n\n\nNow it’s time for the fun part: analyzing the data!\nThis is where we turn all of the hard work of experimental design and setup, data collection, and data processing into (hopefully) insights that we can use to test hypotheses, generate further hypotheses, and contribute to the body of knowledge about a subject!\n\n\nThe data should be visualized before any formal analysis is carried out. Here are just a few examples of the benefits of exploratory analysis:\n\nIdentify issues with data\nIdentify and explore relationships between variables\nGenerate hypotheses\nCheck assumptions of formal hypothesis tests\n\n\n\nR makes it very easy to generate a summary of our data:\n\n# generate a summary of the data\nsummary(b_only)\n\n      Date           PERSON    GRASS      TREATMENT BLOCK    WEIGHT.g       \n Min.   :45504   Boussad:24   Teff:24   Control:6   1:4   Min.   :0.001000  \n 1st Qu.:45504                          Dry    :6   2:4   1st Qu.:0.007175  \n Median :45504                          Liq    :6   3:4   Median :0.010100  \n Mean   :45504                          Nod    :6   4:4   Mean   :0.011575  \n 3rd Qu.:45504                                      5:4   3rd Qu.:0.017250  \n Max.   :45504                                      6:4   Max.   :0.025100  \n\n\nNote that the ‘Mean’ and ‘Max’ weights are well above the ‘3rd Quartile’. This indicates that the data is probably not normally distributed. We will have to check this further in our exploratory analysis.\n\n\n\nContingency tables help us explore the number of samples that are associated with categorical variables. This helps us think about sample sizes and allows us to “sanity check” our data.\nContingency tables may be simple and consider only a single variable:\n\n# simple single variable contingency tables\nxtabs(~ PERSON, data = b_only)\n\nPERSON\nBoussad \n     24 \n\nxtabs(~ GRASS, data = b_only)\n\nGRASS\nTeff \n  24 \n\n\nOr they may consider multiple variables and allow us to check that our samples are distributed as we expect:\n\n# simple single variable contingency tables\nxtabs(~ PERSON + GRASS, data = b_only)\n\n         GRASS\nPERSON    Teff\n  Boussad   24\n\nxtabs(~ BLOCK + GRASS + TREATMENT, data = b_only)\n\n, , TREATMENT = Control\n\n     GRASS\nBLOCK Teff\n    1    1\n    2    1\n    3    1\n    4    1\n    5    1\n    6    1\n\n, , TREATMENT = Dry\n\n     GRASS\nBLOCK Teff\n    1    1\n    2    1\n    3    1\n    4    1\n    5    1\n    6    1\n\n, , TREATMENT = Liq\n\n     GRASS\nBLOCK Teff\n    1    1\n    2    1\n    3    1\n    4    1\n    5    1\n    6    1\n\n, , TREATMENT = Nod\n\n     GRASS\nBLOCK Teff\n    1    1\n    2    1\n    3    1\n    4    1\n    5    1\n    6    1\n\n\nFor example, we see above that all of the Teff grass was collected by Boussad while all the Crabgrass was collected by Zahia.\nAdditionally, we can verify that we have 1 sample from each type of grass, for each treatment, from each block.\nThese sanity checks may seem pointless or dry, but it’s imperative that we verify that we didn’t introduce any errors at any of the steps above.\n\n\n\nA histogram allows us to visualize the distribution of a quantitative variable.\n\nhist(b_only$WEIGHT.g, breaks = 12)\n\n\n\n\n\n\n\n\nAs noted above, it appears that our weight data is not normally distributed.\n\nplot(b_only$BLOCK, b_only$WEIGHT.g)\n\n\n\n\n\n\n\nplot(b_only$TREATMENT, b_only$WEIGHT.g)\n\n\n\n\n\n\n\n\n\n\n\n\nRun a one-tailed ANOVA\n\nfit &lt;- aov(WEIGHT.g ~ BLOCK + TREATMENT, data = b_only)\nsummary(fit)\n\n            Df    Sum Sq   Mean Sq F value Pr(&gt;F)  \nBLOCK        5 0.0002450 4.899e-05   2.339 0.0928 .\nTREATMENT    3 0.0002515 8.382e-05   4.001 0.0281 *\nResiduals   15 0.0003142 2.095e-05                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "basic.html#introduction",
    "href": "basic.html#introduction",
    "title": "Basic Analysis",
    "section": "",
    "text": "This page introduces a brief, basic analysis of grass weights using R.\nThe workflow for a basic analysis consists of two main steps:\n\nPreparing the data — Raw data is read into R and processed into data that is in a format that we can analyze.\nAnalyzing the data — Data is visualized to create plots and tables, and a formal statistical analysis is performed.\n\nWe can visualize the steps of this workflow with a graph:\n\n\n\n\n\nflowchart TB\n    data_file[(\"Data file\")]\n    read_data[Read data\\ninto R]\n    wrangle_data[\"Clean/format\\ndata\"]\n    cleaned_data&gt;\"Analysis-ready\\ndata\"]\n\n    subgraph \"Prepare data\"\n        data_file --- read_data --- wrangle_data --&gt; cleaned_data\n    end\n  \n    viz_data[\"Visualize data\"]\n    visualizations&gt;\"Plots &\\ntables\"]\n    stat_ansys[\"Statistical analysis\"]\n    ansys_res&gt;\"Analysis results\"]\n    \n    subgraph \"Analyze data\"\n        cleaned_data --- viz_data --&gt; visualizations\n        cleaned_data --- stat_ansys --&gt; ansys_res\n    end"
  },
  {
    "objectID": "basic.html#prepare-data",
    "href": "basic.html#prepare-data",
    "title": "Basic Analysis",
    "section": "",
    "text": "This section contains the first of the main steps laid out above: preparing the data for analysis.\nThe goal of this section is to take a raw data file and process it in R so that we have data available in our R session that we can directly use as input for our exploratory and formal data analysis.\n\n\n\nRead data\n\nThe process of loading data from a file, database, or other location meant for long term storage into a short-lived form that we work with in a program.\n\n\nI have made the raw data file in available online on a code and data hosting platform called GitHub at the following URL: https://github.com/utia-gc/20240805-usda-sep-fellows-r-workshop/raw/main/data/b_only.csv. By looking at the file name (grasses.csv) and checking the data ourselves, we can see that this file is in comma separated value (CSV) format.\n\n\n\n\n\n\nWarning\n\n\n\nUsing a file available on the internet is convenient for a task like this workshop. R can directly read certain types of files from the internet, so we don’t have to worry about downloading files, finding where they downloaded on everyone’s computer, and reading them in from the correct location.\nHowever, managing downloads and finding data on your computer is an important part of performing analysis on your own, and it can be tricky for beginners. I recommend that you develop a system for keeping track of all of your data files associated with each project. Personally, I prefer having different folders on my computer for each project, and inside of each project folder I have a data/ folder where I keep all of my data files. But that is just my preference, and you may prefer a different setup. What’s important is finding a way that works for you!\n\n\n\n# read b_only data as csv from URL\ncombined &lt;- read.csv(\"https://github.com/utia-gc/20240805-usda-sep-fellows-r-workshop/raw/main/data/grasses.csv\")\n\nb_only &lt;- combined[combined$PERSON == \"Boussad\", ]\n\n\n\n\n\nClean data\n\nThe act of transforming data from its current format into the structure needed for the intended analysis. This can also include data processing steps such as dropping or filling in missing values, computing new values from the data, etc.\n\n\nOur data is actually already in the format needed for our downstream analyses, so our cleaning steps here are rather light. Mainly, they just help us get categorical variables set up in the order we want for plotting and statistical testing.\nR encodes categorical variables in what it calls factors. These factors are inherently ordered – oftentimes this order doesn’t matter, but it controls what our “reference level” is in some statistical tests. This won’t change the results themselves, but we have to be careful about interpreting the results correctly in terms of the reference level.\n\n\n\n\n\n\nEncoding factors\n\n\n\nWhen we don’t care about the reference level of a factor, we can make the data a factor with the as.factor() function and let R take care of setting the levels for us.\nHowever, when we do care about the reference level, we should set the levels ourselves with the factor() function and make sure to use the levels argument.\n\n\n\n# recode categorical variables as factors one column at a time\nb_only$PERSON &lt;- as.factor(b_only$PERSON)\nb_only$GRASS &lt;- as.factor(b_only$GRASS)\nb_only$BLOCK &lt;- as.factor(b_only$BLOCK)\n\nb_only$TREATMENT &lt;- factor(b_only$TREATMENT, levels = c(\"Control\", \"Dry\", \"Liq\", \"Nod\"))\n\nThis is a good place to talk generally about data cleaning and how to check the data to make sure it’s in the structure that we need.\nTo start, we have to know what format we have, and what format we need. There is no “one size fits all” data structure that we need; different analyses require input data to be structured in different ways. A key part of working with data in R is knowing how to process data from the structure that you have into the structure that you need. This means you must work through tutorials and read the documentation for the analysis tools you work with.\nHowever, one general structure that is very common for analyzing data in R is the data.frame. This is the format of the data that we have read in, and it’s what many functions take as input data.\n\n\n\n\n\n\nVisualizing data in a data.frame\n\n\n\nThe most direct way to figure out how your data is structured is to simply look at it. When you’re interactively performing an analysis in RStudio, I recommend two simple ways of looking at your data by running the following in the Console pane:\n# show the contents of the data.frame directly in the Console\nb_only\n\n# open an interactive view of the data.frame as a table\nView(b_only)"
  },
  {
    "objectID": "basic.html#analyze-data",
    "href": "basic.html#analyze-data",
    "title": "Basic Analysis",
    "section": "",
    "text": "Now it’s time for the fun part: analyzing the data!\nThis is where we turn all of the hard work of experimental design and setup, data collection, and data processing into (hopefully) insights that we can use to test hypotheses, generate further hypotheses, and contribute to the body of knowledge about a subject!\n\n\nThe data should be visualized before any formal analysis is carried out. Here are just a few examples of the benefits of exploratory analysis:\n\nIdentify issues with data\nIdentify and explore relationships between variables\nGenerate hypotheses\nCheck assumptions of formal hypothesis tests\n\n\n\nR makes it very easy to generate a summary of our data:\n\n# generate a summary of the data\nsummary(b_only)\n\n      Date           PERSON    GRASS      TREATMENT BLOCK    WEIGHT.g       \n Min.   :45504   Boussad:24   Teff:24   Control:6   1:4   Min.   :0.001000  \n 1st Qu.:45504                          Dry    :6   2:4   1st Qu.:0.007175  \n Median :45504                          Liq    :6   3:4   Median :0.010100  \n Mean   :45504                          Nod    :6   4:4   Mean   :0.011575  \n 3rd Qu.:45504                                      5:4   3rd Qu.:0.017250  \n Max.   :45504                                      6:4   Max.   :0.025100  \n\n\nNote that the ‘Mean’ and ‘Max’ weights are well above the ‘3rd Quartile’. This indicates that the data is probably not normally distributed. We will have to check this further in our exploratory analysis.\n\n\n\nContingency tables help us explore the number of samples that are associated with categorical variables. This helps us think about sample sizes and allows us to “sanity check” our data.\nContingency tables may be simple and consider only a single variable:\n\n# simple single variable contingency tables\nxtabs(~ PERSON, data = b_only)\n\nPERSON\nBoussad \n     24 \n\nxtabs(~ GRASS, data = b_only)\n\nGRASS\nTeff \n  24 \n\n\nOr they may consider multiple variables and allow us to check that our samples are distributed as we expect:\n\n# simple single variable contingency tables\nxtabs(~ PERSON + GRASS, data = b_only)\n\n         GRASS\nPERSON    Teff\n  Boussad   24\n\nxtabs(~ BLOCK + GRASS + TREATMENT, data = b_only)\n\n, , TREATMENT = Control\n\n     GRASS\nBLOCK Teff\n    1    1\n    2    1\n    3    1\n    4    1\n    5    1\n    6    1\n\n, , TREATMENT = Dry\n\n     GRASS\nBLOCK Teff\n    1    1\n    2    1\n    3    1\n    4    1\n    5    1\n    6    1\n\n, , TREATMENT = Liq\n\n     GRASS\nBLOCK Teff\n    1    1\n    2    1\n    3    1\n    4    1\n    5    1\n    6    1\n\n, , TREATMENT = Nod\n\n     GRASS\nBLOCK Teff\n    1    1\n    2    1\n    3    1\n    4    1\n    5    1\n    6    1\n\n\nFor example, we see above that all of the Teff grass was collected by Boussad while all the Crabgrass was collected by Zahia.\nAdditionally, we can verify that we have 1 sample from each type of grass, for each treatment, from each block.\nThese sanity checks may seem pointless or dry, but it’s imperative that we verify that we didn’t introduce any errors at any of the steps above.\n\n\n\nA histogram allows us to visualize the distribution of a quantitative variable.\n\nhist(b_only$WEIGHT.g, breaks = 12)\n\n\n\n\n\n\n\n\nAs noted above, it appears that our weight data is not normally distributed.\n\nplot(b_only$BLOCK, b_only$WEIGHT.g)\n\n\n\n\n\n\n\nplot(b_only$TREATMENT, b_only$WEIGHT.g)\n\n\n\n\n\n\n\n\n\n\n\n\nRun a one-tailed ANOVA\n\nfit &lt;- aov(WEIGHT.g ~ BLOCK + TREATMENT, data = b_only)\nsummary(fit)\n\n            Df    Sum Sq   Mean Sq F value Pr(&gt;F)  \nBLOCK        5 0.0002450 4.899e-05   2.339 0.0928 .\nTREATMENT    3 0.0002515 8.382e-05   4.001 0.0281 *\nResiduals   15 0.0003142 2.095e-05                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "USDA SEP Fellows R Workshop",
    "section": "",
    "text": "USDA SEP Fellows R Workshop\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]